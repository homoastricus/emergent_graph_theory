import numpy as np
import networkx as nx
import scipy.sparse as sp
import scipy.sparse.linalg as spla
import matplotlib.pyplot as plt
from collections import defaultdict
import math
from collections import deque
from scipy import stats

class UniverseGraphAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –í—Å–µ–ª–µ–Ω–Ω–æ–π —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""

    def __init__(self, N, m, graph_type='RRG', theoretical_N=1e185, theoretical_k=425):
        """
        Parameters:
        N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ (–ø–ª–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —è—á–µ–µ–∫)
        m - —Å—Ç–µ–ø–µ–Ω—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –Ω–∞ —É–∑–µ–ª)
        graph_type - —Ç–∏–ø –≥—Ä–∞—Ñ–∞: 'RRG', 'ER', 'WS' (Watts-Strogatz)
        theoretical_N - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ N –í—Å–µ–ª–µ–Ω–Ω–æ–π
        theoretical_k - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
        """
        self.N = N
        self.m = m
        self.graph_type = graph_type
        self.theoretical_N = theoretical_N
        self.theoretical_k = theoretical_k
        self.G = None
        self.A = None
        self.L = None
        self.results = {}

    def create_graph(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç—å—é"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤): N={self.N}")

        # üîπ 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å
        # k_opt ‚âà log(N) * c, –≥–¥–µ c ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (~3)
        k_opt = int(max(4, np.round(3 * np.log(self.N))))
        #self.m = k_opt
        print(f"  ‚Üí –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Å–≤—è–∑–Ω–æ—Å—Ç—å m = {self.m}")

        # üîπ 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞—Ñ –º–∞–ª—ã—Ö –º–∏—Ä–æ–≤ (Watts‚ÄìStrogatz)
        # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É –Ω–∞–∏–º–µ–Ω—å—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è:
        # p_opt ‚âà 1 / k_opt (–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–∞–ª–∞–Ω—Å –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π)
        p_opt = min(0.1, max(0.005, 1.0 / k_opt))
        print(f"  ‚Üí –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è p = {p_opt:.4f}")

        k_even = self.m if self.m % 2 == 0 else self.m - 1
        self.G = nx.watts_strogatz_graph(n=self.N, k=k_even, p=p_opt, seed=42)

        # üîπ 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–±–æ—Ä –∫—Ä—É–ø–Ω–µ–π—à–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if not nx.is_connected(self.G):
            print("  ‚ö†Ô∏è  –ì—Ä–∞—Ñ –Ω–µ —Å–≤—è–∑–Ω—ã–π ‚Äî –±–µ—Ä—ë–º –Ω–∞–∏–±–æ–ª—å—à—É—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É.")
            largest_cc = max(nx.connected_components(self.G), key=len)
            self.G = self.G.subgraph(largest_cc).copy()
            self.N = self.G.number_of_nodes()
            print(f"  ‚Üí –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —Å N={self.N}")

        # üîπ 4. –°—Ç—Ä–æ–∏–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ª–∞–ø–ª–∞—Å–∏–∞–Ω (–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–π –ø–æ –º–∞—Å—à—Ç–∞–±—É)
        self.A = nx.adjacency_matrix(self.G)
        deg = np.array(self.A.sum(axis=1)).flatten()
        D = sp.diags(deg)
        D_inv_sqrt = sp.diags(1.0 / np.sqrt(np.maximum(deg, 1e-12)))
        self.L = sp.eye(self.N) - D_inv_sqrt @ self.A @ D_inv_sqrt

        # üîπ 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.results['N_final'] = self.N
        self.results['avg_degree'] = np.mean(deg)
        self.results['degree_std'] = np.std(deg)
        self.results['edges_count'] = self.G.number_of_edges()

        print(f"  ‚úì –ì—Ä–∞—Ñ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: N={self.N}, ‚ü®k‚ü©={self.results['avg_degree']:.2f}")

    def compute_volume_scaling_dimension(self, num_samples=1000):
        """
        –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ scaling –æ–±—ä–µ–º–∞: V(r) ~ r^d
        –ò–∑–º–µ—Ä—è–µ–º, –∫–∞–∫ —Ä–∞—Å—Ç–µ—Ç —á–∏—Å–ª–æ —É–∑–ª–æ–≤ –≤ —à–∞—Ä–µ —Ä–∞–¥–∏—É—Å–∞ r –æ—Ç —Å–ª—É—á–∞–π–Ω–æ–π —Ç–æ—á–∫–∏
        """
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ scaling –æ–±—ä–µ–º–∞...")

        try:
            if self.N < 1000:
                return 0

            # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ —Ç–æ—á–∫–∏
            sample_nodes = np.random.choice(self.N, size=min(num_samples, self.N // 10), replace=False)
            radii = list(range(1, 15))  # –†–∞–¥–∏—É—Å—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

            volume_data = []

            for r in radii:
                volumes = []
                for start_node in sample_nodes:
                    # BFS –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –≤—Å–µ—Ö —É–∑–ª–æ–≤ –Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–∏ ‚â§ r
                    visited = set([start_node])
                    queue = deque([(start_node, 0)])

                    while queue:
                        node, distance = queue.popleft()
                        if distance < r:
                            for neighbor in self.G.neighbors(node):
                                if neighbor not in visited:
                                    visited.add(neighbor)
                                    queue.append((neighbor, distance + 1))

                    volumes.append(len(visited))

                if volumes:  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
                    volume_data.append(np.mean(volumes))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            if len(volume_data) < 5:
                return 0

            # –£–±–∏—Ä–∞–µ–º –≤—ã—Ä–æ–∂–¥–µ–Ω–Ω—ã–µ —Å–ª—É—á–∞–∏
            valid_mask = np.array(volume_data) > volume_data[0] + 5

            if np.sum(valid_mask) < 5:
                return 0

            log_r = np.log(np.array(radii)[valid_mask])
            log_V = np.log(np.array(volume_data)[valid_mask])

            # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è: log(V) ~ d * log(r)
            slope, intercept, r_value, p_value, std_err = stats.linregress(log_r, log_V)

            if r_value ** 2 > 0.9 and 0.5 < slope < 8:
                print(f"  –û–±—ä–µ–º–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: d_V = {slope:.3f} (R¬≤ = {r_value ** 2:.3f})")
                return slope
            else:
                return 0

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ–±—ä–µ–º–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0

    def compute_curvature_dimension(self):
        """
        –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ Ollivier-Ricci –∫—Ä–∏–≤–∏–∑–Ω—ã –≥—Ä–∞—Ñ–∞
        –í –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d –∫—Ä–∏–≤–∏–∑–Ω–∞ –∏–º–µ–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã–π scaling
        """
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫—Ä–∏–≤–∏–∑–Ω—É...")

        try:
            if self.N > 5000:  # –î–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É
                sample_size = min(500, self.N // 20)
                sample_nodes = np.random.choice(self.N, size=sample_size, replace=False)
            else:
                sample_nodes = list(self.G.nodes())

            curvatures = []

            for node in sample_nodes:
                neighbors = list(self.G.neighbors(node))
                if len(neighbors) < 2:
                    continue

                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—Ä–∏–≤–∏–∑–Ω—ã —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
                # –í d-–º–µ—Ä–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ: Œ∫ ~ 1/d –¥–ª—è —Å–ª—É—á–∞–π–Ω—ã—Ö –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞—Ñ–æ–≤
                try:
                    subgraph = self.G.subgraph(neighbors + [node])
                    if nx.is_connected(subgraph):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–Ω–æ—Å—Ç—å –ø–æ–¥–≥—Ä–∞—Ñ–∞
                        clustering = nx.transitivity(subgraph)

                        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –≤ –æ—Ü–µ–Ω–∫—É –∫—Ä–∏–≤–∏–∑–Ω—ã
                        if clustering > 0:
                            # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞: Œ∫ ‚âà 2 - 1/clustering –¥–ª—è –º–∞–ª—ã—Ö –º–∏—Ä–æ–≤
                            curvature = 2.0 - 1.0 / clustering
                            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –∫—Ä–∏–≤–∏–∑–Ω—ã
                            if -10 < curvature < 10:
                                curvatures.append(curvature)
                except:
                    continue

            if len(curvatures) < 10:
                return 0

            avg_curvature = np.mean(curvatures)
            std_curvature = np.std(curvatures)

            # –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫—Ä–∏–≤–∏–∑–Ω—É: d ‚âà 1/Œ∫ –¥–ª—è –ø–ª–æ—Å–∫–∏—Ö –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤
            if abs(avg_curvature) > 0.01:
                d_curv = 1.0 / abs(avg_curvature)
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
                if 0.5 < d_curv < 10 and std_curvature / abs(avg_curvature) < 1.0:
                    print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –∫—Ä–∏–≤–∏–∑–Ω—ã: d_Œ∫ = {d_curv:.3f} (Œ∫ = {avg_curvature:.3f})")
                    return d_curv

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏–∑ –∫—Ä–∏–≤–∏–∑–Ω—ã: {e}")

        return 0

    def compute_fractal_dimension(self, num_walks=100, max_steps=200):
        """
        –§—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π —Å–ª—É—á–∞–π–Ω–æ–≥–æ –±–ª—É–∂–¥–∞–Ω–∏—è
        –ò—Å–ø–æ–ª—å–∑—É–µ–º scaling —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–≥–æ —Å–º–µ—â–µ–Ω–∏—è: ‚ü®r¬≤‚ü© ~ t^(2/d_w)
        """
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏...")

        try:
            if self.N < 1000:
                return 0

            # –ú–æ–¥–µ–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –±–ª—É–∂–¥–∞–Ω–∏—è
            msd_data = []  # Mean squared displacement

            # –£–ø—Ä–æ—â–∞–µ–º: –±–µ—Ä–µ–º –º–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            steps_to_test = list(range(5, min(max_steps, 100), 10))

            for step in steps_to_test:
                displacements = []

                for walk in range(num_walks):
                    # –ù–∞—á–∏–Ω–∞–µ–º —Å–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–∑–ª–∞
                    current = np.random.randint(0, self.N)
                    start_node = current

                    # –°–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
                    for s in range(step):
                        neighbors = list(self.G.neighbors(current))
                        if not neighbors:
                            break
                        current = np.random.choice(neighbors)

                    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –Ω–∞—á–∞–ª—å–Ω–æ–π —Ç–æ—á–∫–∏
                    try:
                        if nx.has_path(self.G, start_node, current):
                            distance = nx.shortest_path_length(self.G, start_node, current)
                            displacements.append(distance ** 2)
                    except:
                        continue

                if displacements:
                    msd_data.append((step, np.mean(displacements)))

            if len(msd_data) < 5:
                return 0

            steps = np.array([x[0] for x in msd_data])
            msd = np.array([x[1] for x in msd_data])

            # –§–∏—Ç–∏—Ä—É–µ–º: log(MSD) ~ (2/d_w) * log(t)
            valid_mask = (msd > 0) & (steps > 0)
            if np.sum(valid_mask) < 5:
                return 0

            log_t = np.log(steps[valid_mask])
            log_msd = np.log(msd[valid_mask])

            slope, intercept, r_value, p_value, std_err = stats.linregress(log_t, log_msd)

            if r_value ** 2 > 0.8 and slope > 0:
                d_w = 2.0 / slope  # walk dimension
                # –î–ª—è –æ–±—ã—á–Ω–æ–π –¥–∏—Ñ—Ñ—É–∑–∏–∏: d_f = d_w
                d_fractal = d_w

                if 0.5 < d_fractal < 8:
                    print(f"  –§—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: d_f = {d_fractal:.3f} (R¬≤ = {r_value ** 2:.3f})")
                    return d_fractal

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ —Ñ—Ä–∞–∫—Ç–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")

        return 0

    def compute_spectral_properties(self, k_eig=100):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –º–Ω–æ–≥–æ–º–µ—Ç–æ–¥–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤...")

        k_eig = min(k_eig, self.N - 1)

        try:
            eigvals, eigvecs = spla.eigsh(self.L, k=k_eig, which='SM', maxiter=1000)
            eigvals = np.sort(eigvals)
        except:
            eigvals, eigvecs = spla.eigsh(self.L, k=min(50, k_eig), which='SM')
            eigvals = np.sort(eigvals)

        spectral_gap = eigvals[1] if len(eigvals) > 1 else eigvals[0]

        self.results['spectral_gap'] = spectral_gap
        self.results['eigvals'] = eigvals
        self.results['eigvecs'] = eigvecs

        # –ú–ù–û–ì–û–ú–ï–¢–û–î–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ - –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø
        dimension_estimates = []

        # 1. –û—Å–Ω–æ–≤–Ω–æ–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        d1 = self._estimate_spectral_dimension(eigvals)
        if d1 > 0:
            dimension_estimates.append(d1)

        # 2. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –º–µ—Ç–æ–¥
        d2 = self._estimate_dimension_via_scaling(eigvals)
        if d2 > 0:
            dimension_estimates.append(d2)

        # 3. –ú–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        d3 = self.results.get('rw_spectral_dimension', 0)
        if d3 > 0:
            dimension_estimates.append(d3)

        # 4. –ù–û–í–´–ï –ú–ï–¢–û–î–´ (–≤—ã–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å —Ö–æ—Ä–æ—à–∏–µ –æ—Ü–µ–Ω–∫–∏)
        if len(dimension_estimates) >= 1:  # –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –Ω–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

            # –û–±—ä–µ–º–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            d4 = self.compute_volume_scaling_dimension()
            if d4 > 0:
                dimension_estimates.append(d4)
                self.results['volume_dimension'] = d4

            # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –∫—Ä–∏–≤–∏–∑–Ω—ã
            d5 = self.compute_curvature_dimension()
            if d5 > 0:
                dimension_estimates.append(d5)
                self.results['curvature_dimension'] = d5

            # –§—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            d6 = self.compute_fractal_dimension()
            if d6 > 0:
                dimension_estimates.append(d6)
                self.results['fractal_dimension'] = d6

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        if dimension_estimates:
            final_dimension = np.median(dimension_estimates)
            print(f"  –í—Å–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {[f'{d:.3f}' for d in dimension_estimates]}")
            print(f"  –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {final_dimension:.3f}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–µ—Ç–æ–¥–æ–≤
            self.results['all_dimension_estimates'] = dimension_estimates
            self.results['dimension_std'] = np.std(dimension_estimates)
        else:
            final_dimension = 0
            print(f"  –ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω–∞")

        self.results['spectral_dimension'] = final_dimension
        return spectral_gap

    def _create_mixed_degree_graph(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏ –¥–ª—è –¥—Ä–æ–±–Ω—ã—Ö m"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏ –¥–ª—è m={self.m}")

        # –†–∞–∑–±–∏–≤–∞–µ–º m –Ω–∞ —Ü–µ–ª—É—é –∏ –¥—Ä–æ–±–Ω—É—é —á–∞—Å—Ç–∏
        m_int = int(self.m)
        m_frac = self.m - m_int

        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —É–∑–ª–æ–≤ –±—É–¥—É—Ç –∏–º–µ—Ç—å –ø–æ–≤—ã—à–µ–Ω–Ω—É—é —Å—Ç–µ–ø–µ–Ω—å
        n_high_degree = int(self.N * m_frac)
        n_low_degree = self.N - n_high_degree

        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–µ–ø–µ–Ω–µ–π
        degree_sequence = [m_int] * n_low_degree + [m_int + 1] * n_high_degree

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ç–Ω–æ—Å—Ç—å —Å—É–º–º—ã —Å—Ç–µ–ø–µ–Ω–µ–π
        total_degree = sum(degree_sequence)
        if total_degree % 2 != 0:
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º: —É–±–∏—Ä–∞–µ–º –æ–¥–Ω—É —Å–≤—è–∑—å —É —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–∑–ª–∞
            degree_sequence[0] -= 1
            print(f"–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Å—É–º–º—É —Å—Ç–µ–ø–µ–Ω–µ–π —Å {total_degree} –Ω–∞ {total_degree - 1}")

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ —Å –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é —Å—Ç–µ–ø–µ–Ω–µ–π
        try:
            self.G = nx.configuration_model(degree_sequence, seed=42)
            # –£–±–∏—Ä–∞–µ–º –∫—Ä–∞—Ç–Ω—ã–µ —Ä–µ–±—Ä–∞ –∏ –ø–µ—Ç–ª–∏
            self.G = nx.Graph(self.G)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ
            self.G.remove_edges_from(nx.selfloop_edges(self.G))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏: {e}")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º ER –≥—Ä–∞—Ñ —Å –Ω—É–∂–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é
            p = self.m / (self.N - 1)
            self.G = nx.erdos_renyi_graph(n=self.N, p=p, seed=42)

    def _estimate_dimension_via_scaling(self, eigvals):
        """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scaling –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω–æ–π —á–∞—Å—Ç–∏ —Å–ø–µ–∫—Ç—Ä–∞
            low_freq = eigvals[(eigvals > 0) & (eigvals < np.percentile(eigvals, 40))]

            if len(low_freq) < 8:
                return 0

            # –ú–∞—Å—à—Ç–∞–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            x = np.log(np.arange(1, len(low_freq) + 1))
            y = np.log(low_freq)

            coef = np.polyfit(x, y, 1)
            r2 = np.corrcoef(x, y)[0, 1] ** 2

            if r2 > 0.8:
                # –î–ª—è D-–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: Œª_k ~ k^{2/D}
                d_s = 2 / abs(coef[0])
                return max(0.1, min(10, d_s))

        except:
            pass

        return 0

    def _estimate_spectral_dimension(self, eigvals):
        """–£–°–¢–û–ô–ß–ò–í–ê–Ø –æ—Ü–µ–Ω–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏"""
        if len(eigvals) < 20:
            return 0

        # –ò—Å–∫–ª—é—á–∞–µ–º –Ω—É–ª–µ–≤–æ–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        nonzero_eigvals = eigvals[eigvals > 1e-10]

        if len(nonzero_eigvals) < 15:
            return 0

        # –§–ò–ö–°–ò–†–û–í–ê–ù–ù–´–ô –¥–∏–∞–ø–∞–∑–æ–Ω –Ω–∏–∑–∫–∏—Ö —á–∞—Å—Ç–æ—Ç (–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20% –Ω–µ–Ω—É–ª–µ–≤—ã—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        n_low = max(10, len(nonzero_eigvals) // 5)
        low_eig = nonzero_eigvals[:n_low]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–∑–∏—á–µ—Å–∫—É—é –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç—å –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        if np.max(low_eig) / np.min(low_eig) > 1e6:
            return 0  # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π —Ä–∞–∑–±—Ä–æ—Å - –Ω–µ—Ñ–∏–∑–∏—á–Ω–æ

        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –±–∏–Ω–∞–º–∏
        try:
            # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –±–∏–Ω—ã –æ—Ç min –¥–æ max low_eig
            n_bins = min(12, len(low_eig) // 3)
            log_bins = np.logspace(np.log10(np.min(low_eig)),
                                   np.log10(np.max(low_eig)),
                                   n_bins)

            hist, bin_edges = np.histogram(low_eig, bins=log_bins, density=True)
            bin_centers = np.sqrt(bin_edges[:-1] * bin_edges[1:])

            # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
            valid_mask = (hist > 1e-10) & (bin_centers > 0) & np.isfinite(hist)

            if np.sum(valid_mask) < 5:
                return 0

            log_Œª = np.log(bin_centers[valid_mask])
            log_œÅ = np.log(hist[valid_mask])

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—ã–±—Ä–æ—Å—ã
            z_scores = np.abs((log_œÅ - np.mean(log_œÅ)) / np.std(log_œÅ))
            if np.any(z_scores > 2.5):
                return 0  # –ï—Å—Ç—å –≤—ã–±—Ä–æ—Å—ã

            # –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞
            coef, residuals, _, _, _ = np.polyfit(log_Œª, log_œÅ, 1, full=True)

            if len(residuals) == 0:
                return 0

            # R¬≤ –æ—Ü–µ–Ω–∫–∞
            ss_res = residuals[0]
            ss_tot = np.sum((log_œÅ - np.mean(log_œÅ)) ** 2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

            # –°—Ç—Ä–æ–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            if r_squared < 0.85 or abs(coef[0]) > 8:
                return 0

            d_s = 2 * (coef[0] + 1)

            # –§–ò–ó–ò–ß–ï–°–ö–ò–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø
            if d_s < 0.1 or d_s > 8:
                return 0

            print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: d_s = {d_s:.3f} (R¬≤ = {r_squared:.3f})")
            return d_s

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0

    def compute_information_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")

        # –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —É–∑–µ–ª (–®–µ–Ω–Ω–æ–Ω–æ–≤—Å–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–µ–π)
        degrees = [d for _, d in self.G.degree()]
        degree_probs = np.bincount(degrees) / len(degrees)
        degree_probs = degree_probs[degree_probs > 0]  # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏

        entropy_per_node = -np.sum(degree_probs * np.log2(degree_probs))

        # –ü–æ–ª–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Å–∏—Å—Ç–µ–º—ã
        # –î–ª—è –≥—Ä–∞—Ñ–∞ —Å N —É–∑–ª–∞–º–∏ –∏ E —Ä–µ–±—Ä–∞–º–∏: log2(—á–∏—Å–ª–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π)
        E = self.G.number_of_edges()
        k_possible = self.N * (self.N - 1) // 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–µ–±–µ—Ä

        # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤
        if E < 0.01 * k_possible:
            total_entropy = E * np.log2(k_possible / E) + E * np.log2(math.e)
        else:
            # –û–±—â–∞—è —Ñ–æ—Ä–º—É–ª–∞ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            p = E / k_possible
            if p > 0 and p < 1:
                H_binom = -p * np.log2(p) - (1 - p) * np.log2(1 - p)
                total_entropy = k_possible * H_binom
            else:
                total_entropy = 0

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å (average shortest path length)
        try:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É
            if self.N > 1000:
                sample_nodes = np.random.choice(self.N, size=min(100, self.N // 10), replace=False)
                path_lengths = []
                for i, node1 in enumerate(sample_nodes):
                    for node2 in sample_nodes[i + 1:]:
                        try:
                            length = nx.shortest_path_length(self.G, node1, node2)
                            path_lengths.append(length)
                        except:
                            continue
                avg_path_length = np.mean(path_lengths) if path_lengths else 0
            else:
                avg_path_length = nx.average_shortest_path_length(self.G)
        except:
            avg_path_length = 0

        self.results['entropy_per_node'] = entropy_per_node
        self.results['total_entropy'] = total_entropy
        self.results['information_connectivity'] = avg_path_length

        return entropy_per_node, total_entropy, avg_path_length

    def compute_physical_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫...")

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        d_s_rw = self._estimate_rw_spectral_dimension()
        self.results['rw_spectral_dimension'] = d_s_rw

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (transitivity)
        clustering = nx.transitivity(self.G)
        self.results['clustering_coefficient'] = clustering

        # –ê—Å—Å–æ—Ä—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (assortativity)
        assortativity = nx.degree_assortativity_coefficient(self.G)
        self.results['assortativity'] = assortativity

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è "—Å–∫–æ—Ä–æ—Å—Ç—å —Å–≤–µ—Ç–∞" —á–µ—Ä–µ–∑ spectral gap
        # c_eff ~ 1 / sqrt(Œª‚ÇÅ) –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –µ–¥–∏–Ω–∏—Ü–∞—Ö
        if self.results['spectral_gap'] > 0:
            c_eff = 1.0 / np.sqrt(self.results['spectral_gap'])
            self.results['effective_speed'] = c_eff
        else:
            self.results['effective_speed'] = 0

        return d_s_rw, clustering, assortativity

    def _estimate_rw_spectral_dimension(self, n_steps=1000, sample_size=1000):
        """–û—Ü–µ–Ω–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ"""
        try:
            N = self.N
            if N < 100:
                return 0

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –±–ª—É–∂–¥–∞–Ω–∏—è
            deg = np.array(self.A.sum(axis=1)).flatten()
            D_inv = sp.diags(1 / deg)
            W = D_inv @ self.A

            sample_idx = np.random.choice(N, size=min(sample_size, N // 10), replace=False)
            P0 = np.zeros(N)
            P0[sample_idx] = 1.0 / len(sample_idx)

            P = P0.copy()
            ret_prob = []

            # –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            for t in range(n_steps):
                P = W @ P
                ret_prob.append(P[sample_idx].mean())

            ret_prob = np.array(ret_prob)
            t_vals = np.arange(1, n_steps + 1)

            # –ò—â–µ–º —Å—Ç–µ–ø–µ–Ω–Ω–æ–π –∑–∞–∫–æ–Ω –Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            skip_start = max(10, n_steps // 50)
            if len(t_vals) > skip_start + 50:
                fit_t = t_vals[skip_start:-10]
                fit_P = ret_prob[skip_start:-10]

                if np.all(fit_P > 0) and len(fit_t) > 20:
                    logt = np.log(fit_t)
                    logP = np.log(fit_P)

                    # –ò—Å–∫–ª—é—á–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
                    mask = np.abs(logP - np.mean(logP)) < 2 * np.std(logP)
                    if np.sum(mask) > 10:
                        coef = np.polyfit(logt[mask], logP[mask], 1)
                        d_s = -2 * coef[0]
                        return max(0, d_s)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ—Ü–µ–Ω–∫–µ RW —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")

        return 0

    def add_universe_parameters(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –ï–¢–ò"""

        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –í—Å–µ–ª–µ–Ω–Ω–æ–π
        self.results['N_theoretical'] = self.theoretical_N
        self.results['k_optimal'] = self.theoretical_k
        self.results['target_dimension'] = 3.0
        self.results['correlation_exponent'] = 2.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ —Ç–µ–æ—Ä–∏–∏
        k_ratio = self.results['avg_degree'] / self.results['k_optimal']
        dim_ratio = self.results['spectral_dimension'] / self.results['target_dimension']

        self.results['k_optimality_ratio'] = k_ratio
        self.results['dimension_ratio'] = dim_ratio

    def compute_theoretical_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∞–ª–∏–∑–º–∞ –ï–¢–ò"""

        # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        k = self.results['avg_degree']
        k_opt = self.results['k_optimal']
        k_std = self.results['degree_std']

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –≤–∞—à–µ–π —Ñ–æ—Ä–º—É–ª—ã
        rigidity = k_std ** 2  # (‚àák)¬≤ - –∂–µ—Å—Ç–∫–æ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏–∏
        balance = (k - k_opt) ** 2  # –ë–∞–ª–∞–Ω—Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ —Å–≤–æ–±–æ–¥—ã
        freedom = 1.0 / k ** 2 if k > 0 else 0  # –°–≤–æ–±–æ–¥–∞ (Œ≥/k¬≤)

        # –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è (–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –ø–æ–ª–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏)
        entropy_term = self.results['total_entropy'] / 1e10  # –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞

        # –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
        N = self.results['N_final']
        holo_constraint = N ** (2 / 3) / N  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚àù N^{2/3}

        informational_action = (rigidity + balance + freedom +
                                entropy_term + holo_constraint)

        self.results['informational_action'] = informational_action
        self.results['action_components'] = {
            'rigidity': rigidity,
            'balance': balance,
            'freedom': freedom,
            'entropy': entropy_term,
            'holographic': holo_constraint
        }

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–∫–æ–Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        correlation_exponent = self._estimate_correlation_exponent()
        self.results['estimated_correlation_exponent'] = correlation_exponent
        self.results['correlation_law_deviation'] = abs(correlation_exponent - 2.0)

        # 3. –§–ª—É–∫—Ç—É–∞—Ü–∏–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (œÉ‚Çñ/‚ü®k‚ü© ~ ‚àö‚Ñè)
        fluctuation_ratio = k_std / k if k > 0 else 0
        self.results['connectivity_fluctuation'] = fluctuation_ratio

        return informational_action, correlation_exponent

    def _estimate_correlation_exponent(self):
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞ C(r) ‚àù 1/r^Œ±"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∞—Ç—á–∞–π—à–∏—Ö –ø—É—Ç–µ–π –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            if self.N > 1000:
                sample_nodes = np.random.choice(self.N, size=min(200, self.N // 20), replace=False)
                distances = []
                for i, node1 in enumerate(sample_nodes):
                    for node2 in sample_nodes[i + 1:i + 21]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä—ã
                        try:
                            dist = nx.shortest_path_length(self.G, node1, node2)
                            distances.append(dist)
                        except:
                            continue

                if len(distances) > 50:
                    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
                    hist, bins = np.histogram(distances, bins=20, density=True)
                    bin_centers = (bins[:-1] + bins[1:]) / 2

                    # –§–∏—Ç —Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
                    valid_mask = (hist > 0) & (bin_centers > 0)
                    if np.sum(valid_mask) > 5:
                        log_r = np.log(bin_centers[valid_mask])
                        log_C = np.log(hist[valid_mask])
                        coef = np.polyfit(log_r, log_C, 1)
                        return -coef[0]  # C(r) ‚àù r^{-Œ±}

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ—Ü–µ–Ω–∫–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è: {e}")

        return 0

    def _compute_model_quality(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º"""
        r = self.results

        scores = []

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        k_score = 1.0 - min(1.0, abs(r['k_optimality_ratio'] - 1.0))
        scores.append(k_score * 3)  # –í–µ—Å 3

        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        dim_diff = abs(r['spectral_dimension'] - r['target_dimension'])
        dim_score = 1.0 - min(1.0, dim_diff / r['target_dimension'])
        scores.append(dim_score * 3)  # –í–µ—Å 3

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–∫–æ–Ω
        if 'correlation_law_deviation' in r:
            corr_score = 1.0 - min(1.0, r['correlation_law_deviation'] / 2.0)
            scores.append(corr_score * 2)  # –í–µ—Å 2

        # –§–ª—É–∫—Ç—É–∞—Ü–∏–∏
        fluct_score = 1.0 - min(1.0, abs(r['connectivity_fluctuation'] - 0.01) / 0.01)
        scores.append(fluct_score * 2)  # –í–µ—Å 2

        return min(10, sum(scores))

    def analyze(self, k_eig=100):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞"""
        print("–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –ì–†–ê–§–û–í–û–ô –ú–û–î–ï–õ–ò –í–°–ï–õ–ï–ù–ù–û–ô")

        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞
        self.create_graph()

        # 2. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞
        spectral_gap = self.compute_spectral_properties(k_eig)

        # 3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        entropy_node, entropy_total, connectivity = self.compute_information_metrics()

        # 4. –§–∏–∑–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        d_s_rw, clustering, assortativity = self.compute_physical_metrics()

        # 5. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ï–¢–ò
        self.add_universe_parameters()
        action, corr_exp = self.compute_theoretical_metrics()

        # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_results()
        self._print_theoretical_interpretation()

        return self.results

    def _print_results(self):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")

        results = self.results

        print(f"–û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"  –£–∑–ª–æ–≤ (N): {results['N_final']:,}")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å: {results['avg_degree']:.2f}")
        print(f"  –†–µ–±–µ—Ä: {results['edges_count']:,}")

        print(f"\n–ú–ù–û–ì–û–ú–ï–¢–û–î–ù–ê–Ø –û–¶–ï–ù–ö–ê –†–ê–ó–ú–ï–†–ù–û–°–¢–ò:")
        print(f"  –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['spectral_dimension']:.3f}")

        if 'all_dimension_estimates' in results:
            print(f"  –í—Å–µ –º–µ—Ç–æ–¥—ã: {[f'{d:.3f}' for d in results['all_dimension_estimates']]}")
            print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {results['dimension_std']:.3f}")

        # –ù–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã
        if 'volume_dimension' in results:
            print(f"  –û–±—ä–µ–º–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['volume_dimension']:.3f}")
        if 'curvature_dimension' in results:
            print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–∑ –∫—Ä–∏–≤–∏–∑–Ω—ã: {results['curvature_dimension']:.3f}")
        if 'fractal_dimension' in results:
            print(f"  –§—Ä–∞–∫—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['fractal_dimension']:.3f}")

        print(f"\n–°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –°–í–û–ô–°–¢–í–ê:")
        print(f"  Spectral gap (Œª‚ÇÅ): {results['spectral_gap']:.6f}")
        print(f"  –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['spectral_dimension']:.3f}")
        print(f"  RW —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['rw_spectral_dimension']:.3f}")

        print(f"\n–ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"  –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —É–∑–µ–ª: {results['entropy_per_node']:.3f} –±–∏—Ç")
        print(f"  –ü–æ–ª–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {results['total_entropy']:.3e} –±–∏—Ç")
        print(f"  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: {results['information_connectivity']:.3f}")

        print(f"\n–§–ò–ó–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {results['clustering_coefficient']:.4f}")
        print(f"  –ê—Å—Å–æ—Ä—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: {results['assortativity']:.4f}")
        if 'effective_speed' in results:
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {results['effective_speed']:.4f}")

        print(f"\n–≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –ú–û–î–ï–õ–ò:")
        density = results['edges_count'] / (results['N_final'] * (results['N_final'] - 1) // 2)
        print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞: {density:.6f}")
        print(f"  –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —Å–≤—è–∑—å: {results['entropy_per_node'] / results['avg_degree']:.4f} –±–∏—Ç/—Å–≤—è–∑—å")

    def _print_theoretical_interpretation(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è"""
        print("–¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø (–ï–¢–ò –§–û–†–ú–ê–õ–ò–ó–ú)")

        r = self.results

        print(f"–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï –¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ò–ú –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø–ú:")
        print(f"  ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏: {r['k_optimality_ratio']:.3f} (—Ü–µ–ª—å: 1.0)")
        print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {r['spectral_dimension']:.2f} (—Ü–µ–ª—å: {r['target_dimension']:.1f})")
        print(f"  ‚Ä¢ –§–ª—É–∫—Ç—É–∞—Ü–∏–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏: {r['connectivity_fluctuation']:.4f} (~‚àö‚Ñè)")

        if 'estimated_correlation_exponent' in r:
            print(f"  ‚Ä¢ –ó–∞–∫–æ–Ω –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: C(r) ‚àù 1/r^{r['estimated_correlation_exponent']:.2f} (—Ü–µ–ª—å: 2.00)")

        print(f"\n–ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ï –î–ï–ô–°–¢–í–ò–ï: {r['informational_action']:.6f}")
        if 'action_components' in r:
            comp = r['action_components']
            print(f"  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
            print(f"  ‚Ä¢ –ñ–µ—Å—Ç–∫–æ—Å—Ç—å: {comp['rigidity']:.6f}")
            print(f"  ‚Ä¢ –ë–∞–ª–∞–Ω—Å: {comp['balance']:.6f}")
            print(f"  ‚Ä¢ –°–≤–æ–±–æ–¥–∞: {comp['freedom']:.6f}")
            print(f"  ‚Ä¢ –≠–Ω—Ç—Ä–æ–ø–∏—è: {comp['entropy']:.6f}")
            print(f"  ‚Ä¢ –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ: {comp['holographic']:.6f}")

        # –û—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏
        print(f"\n–§–ò–ó–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò:")
        quality_score = self._compute_model_quality()
        print(f"  –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {quality_score:.1f}/10")

        if quality_score > 7:
            print("  ‚úÖ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º")
        elif quality_score > 5:
            print("  ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        else:
            print("  ‚ùå –ú–æ–¥–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    N = 20000  # 100 —Ç—ã—Å—è—á —É–∑–ª–æ–≤
    m = 220.0 #math.log(N)
    print(f" —Å—Ç–µ–ø–µ–Ω—å —Å–≤—è–∑–∏: {m:.2f}")

    analyzer = UniverseGraphAnalyzer(
        N=N, m=379,
        # (450 –¥–∞–≤–∞–ª–æ 2.6, 410 - 2.827 , 390 - 0, 401 - 2.886,
        # 402-0, 403 - 0, 404 - 2.862, 399 - 0, 398 - 0,
        # 397 - 2.910, 396 - 2.910, 395 - 2.922, 394 - 2.922, 393 - 0, 392 - 0, 391 - 0,
        # 389 - 0, 388 - 0, 387 - 0, 386 - 0,385 - 0, 384 - 0, 383 - 2.998, 382 - 2.998,
        # 381 - 0, 380 - 0, 379 - 3.025  )
        graph_type='RRG',
        theoretical_N=1e185,
        theoretical_k=425
    )

    results = analyzer.analyze()

    print("–§–ò–ó–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –î–õ–Ø –í–°–ï–õ–ï–ù–ù–û–ô:")

    if results['spectral_dimension'] > 0:
        print(f"‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: {results['spectral_dimension']:.2f}")
    print(f"‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –µ–º–∫–æ—Å—Ç—å: {results['total_entropy']:.2e} –±–∏—Ç")
    print(f"‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {results.get('effective_speed', 0):.2f}")
    print(f"‚Ä¢ –°—Ç–µ–ø–µ–Ω—å –∫–≤–∞–Ω—Ç–æ–≤–æ–π –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏: {results['clustering_coefficient']:.3f}")
    print(f"‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {results['k_optimality_ratio']:.3f}")