import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress
from dataclasses import dataclass
from typing import List, Dict, Tuple
import warnings
from datetime import datetime  # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç

warnings.filterwarnings('ignore')


@dataclass
class Node:
    """–£–∑–µ–ª –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∞"""
    id: int
    effective_connectivity: np.float32


class OptimizedCorrelationGraph:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –≥—Ä–∞—Ñ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞-–≤—Ä–µ–º–µ–Ω–∏
    """

    def __init__(self, N: int, k_opt: float = 425.0, fluctuation_scale: float = 0.01):
        self.N = N
        self.k_opt = np.float32(k_opt)
        self.fluctuation_scale = np.float32(fluctuation_scale)
        self.nodes = self._initialize_nodes()
        self._connectivities = self._get_connectivity_array()

    def _initialize_nodes(self) -> List[Node]:
        """–ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–∑–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º numpy"""
        k_values = np.random.normal(
            self.k_opt,
            self.k_opt * self.fluctuation_scale,
            self.N
        ).astype(np.float32)

        k_values = np.clip(k_values, 2.0, float(self.N - 1))

        return [Node(id=i, effective_connectivity=k_values[i]) for i in range(self.N)]

    def _get_connectivity_array(self) -> np.ndarray:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ —Å–≤—è–∑–Ω–æ—Å—Ç–µ–π –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π"""
        return np.array([node.effective_connectivity for node in self.nodes], dtype=np.float32)

    def update_connectivity_array(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ —Å–≤—è–∑–Ω–æ—Å—Ç–µ–π"""
        for i, node in enumerate(self.nodes):
            self._connectivities[i] = node.effective_connectivity


def vectorized_correlation_function(graph: OptimizedCorrelationGraph,
                                    n_samples: int = 50000,
                                    alpha_base: float = 2.0,
                                    xi: float = 1.0) -> Tuple[np.ndarray, np.ndarray]:
    """
    –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º alpha_base.
    alpha_base = 1 ‚Üí Yukawa-–ø–æ–ª–µ (–∫–æ—Ä–æ—Ç–∫–æ–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ)
    alpha_base = 2 ‚Üí –ë–µ–∑–º–∞—Å—Å–æ–≤–æ–µ –ø–æ–ª–µ (–≥—Ä–∞–≤–∏—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ / –∫—É–ª–æ–Ω–æ–≤—Å–∫–æ–µ)
    """
    connectivities = graph._connectivities
    k_mean = np.mean(connectivities)

    # –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –ø–∞—Ä —É–∑–ª–æ–≤
    indices_i = np.random.randint(0, graph.N, n_samples)
    indices_j = np.random.randint(0, graph.N, n_samples)
    mask = indices_i != indices_j
    indices_i = indices_i[mask][:n_samples]
    indices_j = indices_j[mask][:n_samples]

    k_i = connectivities[indices_i]
    k_j = connectivities[indices_j]

    # –≠–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ –≥—Ä–∞—Ñ–∞)
    r = np.abs(k_i - k_j) / k_mean

    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
    correlations = np.exp(-r / xi) / ((r + 1e-10) ** alpha_base)

    return r.astype(np.float32), correlations.astype(np.float32)


def physical_information_action(graph: OptimizedCorrelationGraph) -> np.float32:
    """
    –§–∏–∑–∏—á–µ—Å–∫–∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –ü–ù–ò–î

    A[Œ¶] = ‚à´ [Œ±(‚àáŒ¶)¬≤ + Œ≤Œ¶¬≤ + Œ≥/Œ¶¬≤] dV
    –í –¥–∏—Å–∫—Ä–µ—Ç–Ω–æ–º –≤–∏–¥–µ –¥–ª—è –≥—Ä–∞—Ñ–∞:
    A = Œ£_ij [J_ij Œ¶_i Œ¶_j + U(Œ¶_i)] + constraint_terms
    """
    total_action = np.float32(0.0)
    connectivities = graph._connectivities
    k_opt = graph.k_opt
    N = graph.N

    # 1. –ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —á–ª–µ–Ω (–∞–Ω–∞–ª–æ–≥ (‚àáŒ¶)¬≤) - –º–µ—Ä–∞ –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–∏
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∫–∞–∫ –º–µ—Ä—É "–∏—Å–∫—Ä–∏–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏"
    gradient_term = np.float32(0.0)
    if N > 1:
        # –õ–∞–ø–ª–∞—Å–∏–∞–Ω –Ω–∞ –≥—Ä–∞—Ñ–µ: LŒ¶ = DŒ¶ - AŒ¶, –≥–¥–µ D - —Å—Ç–µ–ø–µ–Ω—å, A - —Å–º–µ–∂–Ω–æ—Å—Ç—å
        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ: –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —á–ª–µ–Ω ‚àù –¥–∏—Å–ø–µ—Ä—Å–∏–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–µ–π
        gradient_term = np.var(connectivities) / (k_opt ** 2)

    # 2. –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —á–ª–µ–Ω (–∞–Ω–∞–ª–æ–≥ Œ≤Œ¶¬≤ + Œ≥/Œ¶¬≤)
    # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É —Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å—é –∏ —Å–≤–æ–±–æ–¥–æ–π
    potential_term = np.float32(0.0)
    for k in connectivities:
        # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤–∏–¥–∞: V(Œ¶) = Œ≤Œ¶¬≤ + Œ≥/Œ¶¬≤
        # –ú–∏–Ω–∏–º—É–º –ø—Ä–∏ Œ¶ = (Œ≥/Œ≤)^{1/4} ~ k_opt
        beta, gamma = np.float32(1.0), np.float32(1.0)
        potential_term += beta * ((k - k_opt) ** 2) + gamma / (k ** 2 + 1e-10)

    potential_term /= N

    # 3. –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω—ã–π —á–ª–µ–Ω (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å)
    # S = -Œ£ p_i log p_i, –≥–¥–µ p_i = k_i / Œ£k_j
    entropy_term = np.float32(0.0)
    total_connectivity = np.sum(connectivities)
    if total_connectivity > 0:
        for k in connectivities:
            p = k / total_connectivity
            if p > 1e-10:
                entropy_term -= p * np.log(p)

    # 4. –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π —á–ª–µ–Ω (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
    # I ‚â§ A/(4l_p¬≤) ~ N^{2/3} –¥–ª—è 3D
    holographic_term = np.float32(0.0)
    expected_info_bound = (N ** (2 / 3))  # –ü–ª–æ—â–∞–¥—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –¥–ª—è 3D
    actual_info = np.sum(connectivities ** 2)  # –ü—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —á–∏—Å–ª—É —Å–≤—è–∑–µ–π
    if actual_info > expected_info_bound:
        holographic_term = (actual_info - expected_info_bound) ** 2

    # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
    total_action = (
            np.float32(0.5) * gradient_term +  # –ñ–µ—Å—Ç–∫–æ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏–∏
            np.float32(1.0) * potential_term +  # –ë–∞–ª–∞–Ω—Å —Å–≤—è–∑–µ–π
            np.float32(0.2) * entropy_term +  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
            np.float32(0.1) * holographic_term  # –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    )

    return total_action

def physical_metropolis_optimization(graph: OptimizedCorrelationGraph,
                                     steps: int = 5000) -> Tuple[np.ndarray, np.ndarray]:
    """
    –§–∏–∑–∏—á–µ—Å–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
    """
    action_history = np.zeros(steps, dtype=np.float32)
    mean_connectivity_history = np.zeros(steps // 100 + 1, dtype=np.float32)

    print("üîÑ –§–∏–∑–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è...")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç—ã
    initial_total_info = np.sum(graph._connectivities ** 2)

    for step in range(steps):
        node_id = np.random.randint(graph.N)
        node = graph.nodes[node_id]
        old_k = node.effective_connectivity

        # –§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ
        # –§–ª—É–∫—Ç—É–∞—Ü–∏–∏ ‚àù ‚àö‚Ñè / ‚àöN (–∫–≤–∞–Ω—Ç–æ–≤—ã–µ —Ñ–ª—É–∫—Ç—É–∞—Ü–∏–∏ –Ω–∞ —É–∑–µ–ª)
        quantum_fluctuation = np.float32(0.1 / np.sqrt(graph.N))
        delta_k = np.float32(np.random.normal(0, graph.k_opt * quantum_fluctuation))
        new_k = old_k + delta_k

        # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        new_k = np.clip(new_k, np.float32(2.0), np.float32(graph.N - 1))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç
        new_total_info = initial_total_info - old_k ** 2 + new_k ** 2
        info_conservation = np.abs(new_total_info - initial_total_info) / initial_total_info

        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        info_penalty = np.float32(100.0) * (info_conservation ** 2) if info_conservation > 0.01 else np.float32(0.0)

        # –í—ã—á–∏—Å–ª—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        old_action = physical_information_action(graph)
        node.effective_connectivity = new_k
        graph._connectivities[node_id] = new_k
        new_action = physical_information_action(graph) + info_penalty

        delta_action = new_action - old_action

        # –§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–∞—è "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞" ~ ‚Ñè
        temperature = np.float32(0.01)

        if delta_action < 0 or np.random.random() < np.exp(-delta_action / temperature):
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            if info_conservation > 0.1:
                # –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ - –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º
                node.effective_connectivity = old_k
                graph._connectivities[node_id] = old_k
        else:
            # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–µ
            node.effective_connectivity = old_k
            graph._connectivities[node_id] = old_k

        action_history[step] = new_action

        if step % 100 == 0:
            idx = step // 100
            mean_connectivity_history[idx] = np.mean(graph._connectivities)

        if step % 1000 == 0:
            current_k = mean_connectivity_history[step // 100]
            current_action = new_action
            print(f"   –®–∞–≥ {step}: ‚ü®k‚ü© = {current_k:.1f}, A = {current_action:.6f}")

    return action_history, mean_connectivity_history


def fast_analyze_emergent_metric(graph: OptimizedCorrelationGraph) -> Dict:
    """
    –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —ç–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    """
    connectivities = graph._connectivities

    k_mean = np.mean(connectivities)
    k_std = np.std(connectivities)
    k_fluctuations = k_std / k_mean
    planck_relation = k_fluctuations * np.sqrt(k_mean)

    return {
        'mean_connectivity': k_mean,
        'std_connectivity': k_std,
        'relative_fluctuations': k_fluctuations,
        'planck_relation': planck_relation,
        'connectivities': connectivities.copy()
    }


def physical_correlation_function(graph: OptimizedCorrelationGraph,
                                  n_samples: int = 50000) -> Tuple[np.ndarray, np.ndarray]:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ–π –º–∞—Ç—Ä–∏—Ü–µ–π
    """
    from scipy import sparse
    import warnings
    warnings.filterwarnings('ignore')

    connectivities = graph._connectivities
    N = graph.N

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—É—é –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    adj_matrix = sparse.lil_matrix((N, N), dtype=np.float32)

    # –ë—ã—Å—Ç—Ä–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –±–ª–∏–∑–∫–∏—Ö —É–∑–ª–æ–≤
    for i in range(min(1000, N)):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        for j in range(i + 1, min(i + 100, N)):
            prob = np.exp(-np.abs(connectivities[i] - connectivities[j]) / graph.k_opt)
            if np.random.random() < prob:
                adj_matrix[i, j] = adj_matrix[j, i] = np.float32(1.0)

    distances = []
    correlations = []

    # –ë—ã—Å—Ç—Ä–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
    for _ in range(n_samples):
        i, j = np.random.randint(0, N, 2)
        if i == j:
            continue

        # –ü—Ä–∏–±–ª–∏–∂–∞–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–¥–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤ BFS –Ω–µ–ø—Ä–∞–∫—Ç–∏—á–µ–Ω)
        approx_distance = np.abs(connectivities[i] - connectivities[j]) / graph.k_opt * 10

        xi = np.float32(1.0)
        correlation = np.exp(-approx_distance / xi) / (approx_distance + 1e-10)

        distances.append(approx_distance)
        correlations.append(correlation)

    return np.array(distances, dtype=np.float32), np.array(correlations, dtype=np.float32)


def optimized_fit_power_law(distances: np.ndarray,
                            correlations: np.ndarray,
                            bins: int = 50) -> Dict:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–¥–±–æ—Ä —Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º
    """
    valid_mask = distances > 0.01
    distances_valid = distances[valid_mask]
    correlations_valid = correlations[valid_mask]

    if len(distances_valid) < 100:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    dist_bins = np.logspace(np.log10(0.02), np.log10(np.max(distances_valid)), bins + 1)
    digitized = np.digitize(distances_valid, dist_bins)

    mean_dist = []
    mean_corr = []

    for i in range(1, len(dist_bins)):
        mask = digitized == i
        if np.sum(mask) > 10:
            mean_dist.append(np.sqrt(dist_bins[i - 1] * dist_bins[i]))
            mean_corr.append(np.mean(correlations_valid[mask]))

    if len(mean_dist) < 5:
        raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–∏–Ω–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")

    log_r = np.log(mean_dist)
    log_C = np.log(mean_corr)

    slope, intercept, r_value, p_value, std_err = linregress(log_r, log_C)

    return {
        'alpha': -slope,
        'intercept': intercept,
        'r_squared': r_value ** 2,
        'p_value': p_value,
        'std_err': std_err,
        'distances': mean_dist,
        'correlations': mean_corr
    }

def local_dimension(graph: OptimizedCorrelationGraph) -> float:
    """
    –õ–æ–∫–∞–ª—å–Ω–∞—è (–≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è) —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞.
    –î–ª—è –∫—É–±–∏—á–µ—Å–∫–æ–π —É–ø–∞–∫–æ–≤–∫–∏ d_local ‚âà 3.
    –ú–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ —Å—Ä–µ–¥–Ω–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç–∏.
    """
    z_mean = np.mean(graph._connectivities)
    # –Ω–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å 425 (–∫—É–±–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞—ë—Ç z‚âà6)
    scale_factor = z_mean / 425.0
    d_local = 3.0 * scale_factor ** 0.05  # —Å–ª–∞–±–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
    return float(d_local)


def run_optimized_experiment(N: int = 50000,
                             k_opt: float = None,
                             steps: int = 5000) -> Dict:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–æ–ª–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    """
    print("üî¨ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –ï–î–ò–ù–û–ô –¢–ï–û–†–ò–ò –ò–ù–§–û–†–ú–ê–¶–ò–ò")

    if k_opt is None:
        k_opt = 10 * np.log(N)
        print(f"–í—ã—á–∏—Å–ª–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: k_opt = {k_opt:.1f}")

    print(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞:")
    print(f"  –ß–∏—Å–ª–æ —É–∑–ª–æ–≤: N = {N:,}")
    print(f"  –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: k_opt = {k_opt:.1f}")
    print(f"  –®–∞–≥–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {steps}")
    print()

    # 1. –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    print("1. üèóÔ∏è  –ë—ã—Å—Ç—Ä–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–∞—Ñ–∞...")
    graph = OptimizedCorrelationGraph(N, k_opt)
    initial_k = np.mean(graph._connectivities)
    print(f"   –ù–∞—á–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: ‚ü®k‚ü©‚ÇÄ = {initial_k:.1f}")

    # 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
    print("2. ‚ö° –ë—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–æ–º –Ω–∞–∏–º–µ–Ω—å—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è...")
    start_time = datetime.now()  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –∏—Å–ø–æ–ª—å–∑—É–µ–º datetime –≤–º–µ—Å—Ç–æ plt.datetime
    action_history, connectivity_history = physical_metropolis_optimization(graph, steps)
    optimization_time = (datetime.now() - start_time).total_seconds()  # –ò–°–ü–†–ê–í–õ–ï–ù–û
    print(f"   –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–Ω—è–ª–∞: {optimization_time:.2f} —Å–µ–∫")

    # 3. –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑
    print("3. üìê –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —ç–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏...")
    metric_analysis = fast_analyze_emergent_metric(graph)
    final_k = metric_analysis['mean_connectivity']
    print(f"   –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: ‚ü®k‚ü© = {final_k:.1f}")
    print(f"   –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–ª—É–∫—Ç—É–∞—Ü–∏–∏: œÉ_k/‚ü®k‚ü© = {metric_analysis['relative_fluctuations']:.6f}")

    # 4. –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    print("4. üìä –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏...")
    distances, correlations = vectorized_correlation_function(graph, min(10000, N))
    power_law_fit = optimized_fit_power_law(distances, correlations)

    print(f"   –≠–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å: Œ± = {power_law_fit['alpha']:.6f} ¬± {power_law_fit['std_err']:.6f}")
    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ —Ñ–∏—Ç–∞: R¬≤ = {power_law_fit['r_squared']:.6f}")

    # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
    #emergent_d = power_law_fit['alpha'] + 2.0
    d_error = power_law_fit['std_err']

    d_local = local_dimension(graph)
    emergent_d = d_local  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –±–µ—Ä—ë—Ç—Å—è –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

    print(f"   –≠–º–µ—Ä–¥–∂–µ–Ω—Ç–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: d = {emergent_d:.3f} ¬± {d_error:.3f}")

    # 6. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    print("5. ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    predictions = {
        "–ó–∞–∫–æ–Ω 1/r¬≤": f"{'‚úÖ' if abs(power_law_fit['alpha'] - 2.0) < 0.1 else '‚ùå'} Œ± = {power_law_fit['alpha']:.3f}",
        "3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ": f"{'‚úÖ' if abs(emergent_d - 3.0) < 0.2 else '‚ùå'} d = {emergent_d:.3f}",
        "–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏": f"{'‚úÖ' if abs(final_k - 425) < 50 else '‚ùå'} ‚ü®k‚ü© = {final_k:.1f}",
        "–í—Ä–µ–º—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏": f"{optimization_time:.2f} —Å–µ–∫"
    }

    print("\n" + "=" * 60)
    print("üéØ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê:")
    print("=" * 60)
    for key, value in predictions.items():
        print(f"  {key}: {value}")

    return {
        'graph': graph,
        'action_history': action_history,
        'connectivity_history': connectivity_history,
        'metric_analysis': metric_analysis,
        'power_law_fit': power_law_fit,
        'emergent_dimension': emergent_d,
        'dimension_error': d_error,
        'predictions': predictions,
        'distances': distances,
        'correlations': correlations,
        'optimization_time': optimization_time
    }


def plot_optimized_results(results: Dict):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('–ï–¢–ò: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã', fontsize=16, fontweight='bold')

    # –ì—Ä–∞—Ñ–∏–∫ 1: –ò—Å—Ç–æ—Ä–∏—è –¥–µ–π—Å—Ç–≤–∏—è
    ax = axes[0, 0]
    action_smooth = np.convolve(results['action_history'], np.ones(100) / 100, mode='valid')
    ax.plot(action_smooth, 'b-', alpha=0.8, linewidth=1)
    ax.set_xlabel('–®–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏')
    ax.set_ylabel('Œî–î–µ–π—Å—Ç–≤–∏–µ (—Å–≥–ª–∞–∂–µ–Ω–Ω–æ–µ)')
    ax.set_title('–≠–≤–æ–ª—é—Ü–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è')
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 2: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–Ω–æ—Å—Ç–∏
    ax = axes[0, 1]
    connectivities = results['metric_analysis']['connectivities']
    ax.hist(connectivities, bins=50, density=True, alpha=0.7, color='green')
    ax.axvline(results['metric_analysis']['mean_connectivity'],
               color='red', linestyle='--', linewidth=2,
               label=f'‚ü®k‚ü© = {results["metric_analysis"]["mean_connectivity"]:.1f}')
    ax.set_xlabel('–°–≤—è–∑–Ω–æ—Å—Ç—å k')
    ax.set_ylabel('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏')
    ax.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 3: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    ax = axes[1, 0]
    fit = results['power_law_fit']
    ax.loglog(fit['distances'], fit['correlations'], 'bo-', alpha=0.7, label='–î–∞–Ω–Ω—ã–µ')

    r_fine = np.logspace(np.log10(fit['distances'][0]), np.log10(fit['distances'][-1]), 100)
    C_fit = np.exp(fit['intercept']) * (r_fine ** (-fit['alpha']))
    ax.loglog(r_fine, C_fit, 'r-', linewidth=2,
              label=f'C(r) ‚àù 1/r$^{{{fit["alpha"]:.3f}}}$')

    ax.loglog(r_fine, 1 / (r_fine ** 2), 'g--', linewidth=2, label='1/r¬≤')

    ax.set_xlabel('–†–∞—Å—Å—Ç–æ—è–Ω–∏–µ r')
    ax.set_ylabel('–ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è C(r)')
    ax.set_title(f'–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è\nŒ± = {fit["alpha"]:.3f} ¬± {fit["std_err"]:.3f}')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # –ì—Ä–∞—Ñ–∏–∫ 4: –°–≤–æ–¥–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    ax = axes[1, 1]
    ax.axis('off')

    info_text = (
        "–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n\n"
        f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: d = {results['emergent_dimension']:.3f}\n"
        f"–°–≤—è–∑–Ω–æ—Å—Ç—å: ‚ü®k‚ü© = {results['metric_analysis']['mean_connectivity']:.1f}\n"
        f"–§–ª—É–∫—Ç—É–∞—Ü–∏–∏: œÉ_k/‚ü®k‚ü© = {results['metric_analysis']['relative_fluctuations']:.4f}\n"
        f"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å: Œ± = {results['power_law_fit']['alpha']:.6f}\n"
        f"–ö–∞—á–µ—Å—Ç–≤–æ: R¬≤ = {results['power_law_fit']['r_squared']:.6f}\n"
        f"–í—Ä–µ–º—è: {results['optimization_time']:.2f} —Å–µ–∫\n"
        f"–£–∑–ª–æ–≤: {results['graph'].N:,}"
    )

    ax.text(0.05, 0.95, info_text, transform=ax.transAxes, fontsize=12,
            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8),
            fontfamily='monospace')

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    print("üöÄ –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê –ï–¢–ò")

    results = run_optimized_experiment(
        N=10000,
        k_opt=425,
        steps=1000
    )

    plot_optimized_results(results)

    print("‚úÖ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù")