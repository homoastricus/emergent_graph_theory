import numpy as np
import networkx as nx
import scipy.sparse as sp
import scipy.sparse.linalg as spla
import matplotlib.pyplot as plt
from collections import defaultdict
import math


class UniverseGraphAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–æ–≤–æ–π –º–æ–¥–µ–ª–∏ –í—Å–µ–ª–µ–Ω–Ω–æ–π —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""

    def __init__(self, N, m, graph_type='RRG', theoretical_N=1e185, theoretical_k=425):
        """
        Parameters:
        N - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ (–ø–ª–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —è—á–µ–µ–∫)
        m - —Å—Ç–µ–ø–µ–Ω—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –Ω–∞ —É–∑–µ–ª)
        graph_type - —Ç–∏–ø –≥—Ä–∞—Ñ–∞: 'RRG', 'ER', 'WS' (Watts-Strogatz)
        theoretical_N - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ N –í—Å–µ–ª–µ–Ω–Ω–æ–π
        theoretical_k - —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å
        """
        self.N = N
        self.m = m
        self.graph_type = graph_type
        self.theoretical_N = theoretical_N
        self.theoretical_k = theoretical_k
        self.G = None
        self.A = None
        self.L = None
        self.results = {}

    def create_graph(self):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ–π —Å–≤—è–∑–Ω–æ—Å—Ç—å—é"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤): N={self.N}")

        # üîπ 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å–≤—è–∑–Ω–æ—Å—Ç—å
        # k_opt ‚âà log(N) * c, –≥–¥–µ c ‚Äî –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (~3)
        k_opt = int(max(4, np.round(3 * np.log(self.N))))
        #self.m = k_opt
        print(f"  ‚Üí –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±—Ä–∞–Ω–∞ —Å–≤—è–∑–Ω–æ—Å—Ç—å m = {self.m}")

        # üîπ 2. –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞—Ñ –º–∞–ª—ã—Ö –º–∏—Ä–æ–≤ (Watts‚ÄìStrogatz)
        # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –ø–æ–¥–±–∏—Ä–∞–µ—Ç—Å—è –ø–æ –ø—Ä–∏–Ω—Ü–∏–ø—É –Ω–∞–∏–º–µ–Ω—å—à–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è:
        # p_opt ‚âà 1 / k_opt (–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –±–∞–ª–∞–Ω—Å –ª–æ–∫–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π)
        p_opt = min(0.1, max(0.005, 1.0 / k_opt))
        print(f"  ‚Üí –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è p = {p_opt:.4f}")

        k_even = self.m if self.m % 2 == 0 else self.m - 1
        self.G = nx.watts_strogatz_graph(n=self.N, k=k_even, p=p_opt, seed=42)

        # üîπ 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–±–æ—Ä –∫—Ä—É–ø–Ω–µ–π—à–µ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        if not nx.is_connected(self.G):
            print("  ‚ö†Ô∏è  –ì—Ä–∞—Ñ –Ω–µ —Å–≤—è–∑–Ω—ã–π ‚Äî –±–µ—Ä—ë–º –Ω–∞–∏–±–æ–ª—å—à—É—é –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É.")
            largest_cc = max(nx.connected_components(self.G), key=len)
            self.G = self.G.subgraph(largest_cc).copy()
            self.N = self.G.number_of_nodes()
            print(f"  ‚Üí –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—É —Å N={self.N}")

        # üîπ 4. –°—Ç—Ä–æ–∏–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –ª–∞–ø–ª–∞—Å–∏–∞–Ω (–∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–Ω—ã–π –ø–æ –º–∞—Å—à—Ç–∞–±—É)
        self.A = nx.adjacency_matrix(self.G)
        deg = np.array(self.A.sum(axis=1)).flatten()
        D = sp.diags(deg)
        D_inv_sqrt = sp.diags(1.0 / np.sqrt(np.maximum(deg, 1e-12)))
        self.L = sp.eye(self.N) - D_inv_sqrt @ self.A @ D_inv_sqrt

        # üîπ 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.results['N_final'] = self.N
        self.results['avg_degree'] = np.mean(deg)
        self.results['degree_std'] = np.std(deg)
        self.results['edges_count'] = self.G.number_of_edges()

        print(f"  ‚úì –ì—Ä–∞—Ñ —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω: N={self.N}, ‚ü®k‚ü©={self.results['avg_degree']:.2f}")

    def _create_mixed_degree_graph(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏ –¥–ª—è –¥—Ä–æ–±–Ω—ã—Ö m"""
        print(f"–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏ –¥–ª—è m={self.m}")

        # –†–∞–∑–±–∏–≤–∞–µ–º m –Ω–∞ —Ü–µ–ª—É—é –∏ –¥—Ä–æ–±–Ω—É—é —á–∞—Å—Ç–∏
        m_int = int(self.m)
        m_frac = self.m - m_int

        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —É–∑–ª–æ–≤ –±—É–¥—É—Ç –∏–º–µ—Ç—å –ø–æ–≤—ã—à–µ–Ω–Ω—É—é —Å—Ç–µ–ø–µ–Ω—å
        n_high_degree = int(self.N * m_frac)
        n_low_degree = self.N - n_high_degree

        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç–µ–ø–µ–Ω–µ–π
        degree_sequence = [m_int] * n_low_degree + [m_int + 1] * n_high_degree

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ç–Ω–æ—Å—Ç—å —Å—É–º–º—ã —Å—Ç–µ–ø–µ–Ω–µ–π
        total_degree = sum(degree_sequence)
        if total_degree % 2 != 0:
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º: —É–±–∏—Ä–∞–µ–º –æ–¥–Ω—É —Å–≤—è–∑—å —É —Å–ª—É—á–∞–π–Ω–æ–≥–æ —É–∑–ª–∞
            degree_sequence[0] -= 1
            print(f"–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Å—É–º–º—É —Å—Ç–µ–ø–µ–Ω–µ–π —Å {total_degree} –Ω–∞ {total_degree - 1}")

        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ —Å –∑–∞–¥–∞–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é —Å—Ç–µ–ø–µ–Ω–µ–π
        try:
            self.G = nx.configuration_model(degree_sequence, seed=42)
            # –£–±–∏—Ä–∞–µ–º –∫—Ä–∞—Ç–Ω—ã–µ —Ä–µ–±—Ä–∞ –∏ –ø–µ—Ç–ª–∏
            self.G = nx.Graph(self.G)  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø—Ä–æ—Å—Ç–æ–π –≥—Ä–∞—Ñ
            self.G.remove_edges_from(nx.selfloop_edges(self.G))
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≥—Ä–∞—Ñ–∞ —Å–æ —Å–º–µ—à–∞–Ω–Ω—ã–º–∏ —Å—Ç–µ–ø–µ–Ω—è–º–∏: {e}")
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º ER –≥—Ä–∞—Ñ —Å –Ω—É–∂–Ω–æ–π –ø–ª–æ—Ç–Ω–æ—Å—Ç—å—é
            p = self.m / (self.N - 1)
            self.G = nx.erdos_renyi_graph(n=self.N, p=p, seed=42)

    def compute_spectral_properties(self, k_eig=100):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤...")

        k_eig = min(k_eig, self.N - 1)

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
        try:
            eigvals, eigvecs = spla.eigsh(self.L, k=k_eig, which='SM', maxiter=1000)
            eigvals = np.sort(eigvals)
        except:
            # Fallback –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
            eigvals, eigvecs = spla.eigsh(self.L, k=min(50, k_eig), which='SM')
            eigvals = np.sort(eigvals)

        spectral_gap = eigvals[1] if len(eigvals) > 1 else eigvals[0]

        self.results['spectral_gap'] = spectral_gap
        self.results['eigvals'] = eigvals
        self.results['eigvecs'] = eigvecs

        # –ú–ù–û–ì–û–ú–ï–¢–û–î–ù–ê–Ø –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        dimension_estimates = []

        # 1. –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥
        d1 = self._estimate_spectral_dimension(eigvals)
        if d1 > 0:
            dimension_estimates.append(d1)

        # 2. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ scaling
        d2 = self._estimate_dimension_via_scaling(eigvals)
        if d2 > 0:
            dimension_estimates.append(d2)

        # 3. –ú–µ—Ç–æ–¥ —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        d3 = self.results.get('rw_spectral_dimension', 0)
        if d3 > 0:
            dimension_estimates.append(d3)

        # –£—Å—Ä–µ–¥–Ω—è–µ–º –Ω–∞–¥–µ–∂–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏
        if dimension_estimates:
            final_dimension = np.median(dimension_estimates)
            print(f"  –û—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {dimension_estimates}")
            print(f"  –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {final_dimension:.3f}")
        else:
            final_dimension = 0
            print(f"  –ù–∞–¥–µ–∂–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–µ –ø–æ–ª—É—á–µ–Ω–∞")

        self.results['spectral_dimension'] = final_dimension
        return spectral_gap

    def _estimate_spectral_dimension(self, eigvals):
        """
        –°—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_s —á–µ—Ä–µ–∑ scaling –Ω–∏–∑–∫–∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ Œª_k ~ k^(2/d_s).
        """
        # –£–¥–∞–ª—è–µ–º –Ω—É–ª–∏
        nonzero = eigvals[eigvals > 1e-12]
        if len(nonzero) < 10:
            return 0

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –Ω–∏–∑–∫–æ—á–∞—Å—Ç–æ—Ç–Ω—É—é —á–∞—Å—Ç—å —Å–ø–µ–∫—Ç—Ä–∞
        M = min(100, len(nonzero))
        low = np.sort(nonzero[:M])
        k = np.arange(1, len(low) + 1)

        log_k = np.log(k)
        log_lambda = np.log(low)

        # –õ–∏–Ω–µ–π–Ω–∞—è –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è log(Œª_k) ~ (2/d_s)*log(k)
        slope, intercept = np.polyfit(log_k, log_lambda, 1)
        d_s = 2.0 / slope if slope != 0 else 0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏
        residuals = log_lambda - (slope * log_k + intercept)
        ss_res = np.sum(residuals**2)
        ss_tot = np.sum((log_lambda - np.mean(log_lambda))**2)
        r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0

        # –°—Ç—Ä–æ–≥–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if r2 < 0.85 or d_s < 0.1 or d_s > 10:
            return 0

        print(f"  –°—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: d_s = {d_s:.3f} (R¬≤ = {r2:.3f})")
        return d_s


    def _estimate_dimension_via_scaling(self, L=None, M=100):
        """
        –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ d_s –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –ª–∞–ø–ª–∞—Å–∏–∞–Ω–∞ L (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω),
        –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ –∂–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã scaling –Ω–∏–∑–∫–∏—Ö —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π.
        """
        try:
            if L is None:
                L = self.L

            N = L.shape[0]
            M = min(M, N - 1)
            eigvals, _ = spla.eigsh(L, k=M, which='SM', maxiter=2000)
            eigvals = np.sort(eigvals)
            nonzero = eigvals[eigvals > 1e-12]
            if len(nonzero) < 5:
                return 0

            k = np.arange(1, len(nonzero) + 1)
            log_k = np.log(k)
            log_lambda = np.log(nonzero)

            slope, intercept = np.polyfit(log_k, log_lambda, 1)
            d_s = 2.0 / slope if slope != 0 else 0

            residuals = log_lambda - (slope * log_k + intercept)
            ss_res = np.sum(residuals**2)
            ss_tot = np.sum((log_lambda - np.mean(log_lambda))**2)
            r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0

            if r2 < 0.85 or d_s < 0.1 or d_s > 10:
                return 0

            print(f"  –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è —Å—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞: d_s = {d_s:.3f} (R¬≤ = {r2:.3f})")
            return d_s

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ç—Ä–æ–≥–æ–π –æ—Ü–µ–Ω–∫–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")
            return 0


    def compute_information_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫...")

        # –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —É–∑–µ–ª (–®–µ–Ω–Ω–æ–Ω–æ–≤—Å–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–µ–π)
        degrees = [d for _, d in self.G.degree()]
        degree_probs = np.bincount(degrees) / len(degrees)
        degree_probs = degree_probs[degree_probs > 0]  # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏

        entropy_per_node = -np.sum(degree_probs * np.log2(degree_probs))

        # –ü–æ–ª–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è —Å–∏—Å—Ç–µ–º—ã
        # –î–ª—è –≥—Ä–∞—Ñ–∞ —Å N —É–∑–ª–∞–º–∏ –∏ E —Ä–µ–±—Ä–∞–º–∏: log2(—á–∏—Å–ª–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π)
        E = self.G.number_of_edges()
        k_possible = self.N * (self.N - 1) // 2  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Ä–µ–±–µ—Ä

        # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–∏–µ –¥–ª—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –≥—Ä–∞—Ñ–æ–≤
        if E < 0.01 * k_possible:
            total_entropy = E * np.log2(k_possible / E) + E * np.log2(math.e)
        else:
            # –û–±—â–∞—è —Ñ–æ—Ä–º—É–ª–∞ —á–µ—Ä–µ–∑ —ç–Ω—Ç—Ä–æ–ø–∏—é –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            p = E / k_possible
            if p > 0 and p < 1:
                H_binom = -p * np.log2(p) - (1 - p) * np.log2(1 - p)
                total_entropy = k_possible * H_binom
            else:
                total_entropy = 0

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å (average shortest path length)
        try:
            # –î–ª—è –±–æ–ª—å—à–∏—Ö –≥—Ä–∞—Ñ–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É
            if self.N > 1000:
                sample_nodes = np.random.choice(self.N, size=min(100, self.N // 10), replace=False)
                path_lengths = []
                for i, node1 in enumerate(sample_nodes):
                    for node2 in sample_nodes[i + 1:]:
                        try:
                            length = nx.shortest_path_length(self.G, node1, node2)
                            path_lengths.append(length)
                        except:
                            continue
                avg_path_length = np.mean(path_lengths) if path_lengths else 0
            else:
                avg_path_length = nx.average_shortest_path_length(self.G)
        except:
            avg_path_length = 0

        self.results['entropy_per_node'] = entropy_per_node
        self.results['total_entropy'] = total_entropy
        self.results['information_connectivity'] = avg_path_length

        return entropy_per_node, total_entropy, avg_path_length

    def compute_physical_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        print("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫...")

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ
        d_s_rw = self._estimate_rw_spectral_dimension()
        self.results['rw_spectral_dimension'] = d_s_rw

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (transitivity)
        clustering = nx.transitivity(self.G)
        self.results['clustering_coefficient'] = clustering

        # –ê—Å—Å–æ—Ä—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å (assortativity)
        assortativity = nx.degree_assortativity_coefficient(self.G)
        self.results['assortativity'] = assortativity

        # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è "—Å–∫–æ—Ä–æ—Å—Ç—å —Å–≤–µ—Ç–∞" —á–µ—Ä–µ–∑ spectral gap
        # c_eff ~ 1 / sqrt(Œª‚ÇÅ) –≤ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –µ–¥–∏–Ω–∏—Ü–∞—Ö
        if self.results['spectral_gap'] > 0:
            c_eff = 1.0 / np.sqrt(self.results['spectral_gap'])
            self.results['effective_speed'] = c_eff
        else:
            self.results['effective_speed'] = 0

        return d_s_rw, clustering, assortativity

    def _estimate_rw_spectral_dimension(self, n_steps=1000, sample_size=1000):
        """–û—Ü–µ–Ω–∫–∞ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ —Å–ª—É—á–∞–π–Ω–æ–µ –±–ª—É–∂–¥–∞–Ω–∏–µ"""
        try:
            N = self.N
            if N < 100:
                return 0

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –±–ª—É–∂–¥–∞–Ω–∏—è
            deg = np.array(self.A.sum(axis=1)).flatten()
            D_inv = sp.diags(1 / deg)
            W = D_inv @ self.A

            sample_idx = np.random.choice(N, size=min(sample_size, N // 10), replace=False)
            P0 = np.zeros(N)
            P0[sample_idx] = 1.0 / len(sample_idx)

            P = P0.copy()
            ret_prob = []

            # –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            for t in range(n_steps):
                P = W @ P
                ret_prob.append(P[sample_idx].mean())

            ret_prob = np.array(ret_prob)
            t_vals = np.arange(1, n_steps + 1)

            # –ò—â–µ–º —Å—Ç–µ–ø–µ–Ω–Ω–æ–π –∑–∞–∫–æ–Ω –Ω–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            skip_start = max(10, n_steps // 50)
            if len(t_vals) > skip_start + 50:
                fit_t = t_vals[skip_start:-10]
                fit_P = ret_prob[skip_start:-10]

                if np.all(fit_P > 0) and len(fit_t) > 20:
                    logt = np.log(fit_t)
                    logP = np.log(fit_P)

                    # –ò—Å–∫–ª—é—á–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
                    mask = np.abs(logP - np.mean(logP)) < 2 * np.std(logP)
                    if np.sum(mask) > 10:
                        coef = np.polyfit(logt[mask], logP[mask], 1)
                        d_s = -2 * coef[0]
                        return max(0, d_s)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ—Ü–µ–Ω–∫–µ RW —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏: {e}")

        return 0

    def add_universe_parameters(self):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –ï–¢–ò"""

        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –í—Å–µ–ª–µ–Ω–Ω–æ–π
        self.results['N_theoretical'] = self.theoretical_N
        self.results['k_optimal'] = self.theoretical_k
        self.results['target_dimension'] = 3.0
        self.results['correlation_exponent'] = 2.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ —Ç–µ–æ—Ä–∏–∏
        k_ratio = self.results['avg_degree'] / self.results['k_optimal']
        dim_ratio = self.results['spectral_dimension'] / self.results['target_dimension']

        self.results['k_optimality_ratio'] = k_ratio
        self.results['dimension_ratio'] = dim_ratio

    def compute_theoretical_metrics(self):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∏–∑ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ñ–æ—Ä–º–∞–ª–∏–∑–º–∞ –ï–¢–ò"""

        # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        k = self.results['avg_degree']
        k_opt = self.results['k_optimal']
        k_std = self.results['degree_std']

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–µ–π—Å—Ç–≤–∏—è –∏–∑ –≤–∞—à–µ–π —Ñ–æ—Ä–º—É–ª—ã
        rigidity = k_std ** 2  # (‚àák)¬≤ - –∂–µ—Å—Ç–∫–æ—Å—Ç—å –≥–µ–æ–º–µ—Ç—Ä–∏–∏
        balance = (k - k_opt) ** 2  # –ë–∞–ª–∞–Ω—Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ —Å–≤–æ–±–æ–¥—ã
        freedom = 1.0 / k ** 2 if k > 0 else 0  # –°–≤–æ–±–æ–¥–∞ (Œ≥/k¬≤)

        # –≠–Ω—Ç—Ä–æ–ø–∏–π–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è (–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–∞ –ø–æ–ª–Ω–æ–π —ç–Ω—Ç—Ä–æ–ø–∏–∏)
        entropy_term = self.results['total_entropy'] / 1e10  # –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞

        # –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
        N = self.results['N_final']
        holo_constraint = N ** (2 / 3) / N  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ‚àù N^{2/3}

        informational_action = (rigidity + balance + freedom +
                                entropy_term + holo_constraint)

        self.results['informational_action'] = informational_action
        self.results['action_components'] = {
            'rigidity': rigidity,
            'balance': balance,
            'freedom': freedom,
            'entropy': entropy_term,
            'holographic': holo_constraint
        }

        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–∫–æ–Ω–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        correlation_exponent = self._estimate_correlation_exponent()
        self.results['estimated_correlation_exponent'] = correlation_exponent
        self.results['correlation_law_deviation'] = abs(correlation_exponent - 2.0)

        # 3. –§–ª—É–∫—Ç—É–∞—Ü–∏–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (œÉ‚Çñ/‚ü®k‚ü© ~ ‚àö‚Ñè)
        fluctuation_ratio = k_std / k if k > 0 else 0
        self.results['connectivity_fluctuation'] = fluctuation_ratio

        return informational_action, correlation_exponent

    def _estimate_correlation_exponent(self):
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞ C(r) ‚àù 1/r^Œ±"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—Ä–∞—Ç—á–∞–π—à–∏—Ö –ø—É—Ç–µ–π –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            if self.N > 1000:
                sample_nodes = np.random.choice(self.N, size=min(200, self.N // 20), replace=False)
                distances = []
                for i, node1 in enumerate(sample_nodes):
                    for node2 in sample_nodes[i + 1:i + 21]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä—ã
                        try:
                            dist = nx.shortest_path_length(self.G, node1, node2)
                            distances.append(dist)
                        except:
                            continue

                if len(distances) > 50:
                    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
                    hist, bins = np.histogram(distances, bins=20, density=True)
                    bin_centers = (bins[:-1] + bins[1:]) / 2

                    # –§–∏—Ç —Å—Ç–µ–ø–µ–Ω–Ω–æ–≥–æ –∑–∞–∫–æ–Ω–∞ (–∏—Å–∫–ª—é—á–∞—è –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
                    valid_mask = (hist > 0) & (bin_centers > 0)
                    if np.sum(valid_mask) > 5:
                        log_r = np.log(bin_centers[valid_mask])
                        log_C = np.log(hist[valid_mask])
                        coef = np.polyfit(log_r, log_C, 1)
                        return -coef[0]  # C(r) ‚àù r^{-Œ±}

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ –æ—Ü–µ–Ω–∫–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è: {e}")

        return 0

    def _compute_model_quality(self):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º"""
        r = self.results

        scores = []

        # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏
        k_score = 1.0 - min(1.0, abs(r['k_optimality_ratio'] - 1.0))
        scores.append(k_score * 3)  # –í–µ—Å 3

        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        dim_diff = abs(r['spectral_dimension'] - r['target_dimension'])
        dim_score = 1.0 - min(1.0, dim_diff / r['target_dimension'])
        scores.append(dim_score * 3)  # –í–µ—Å 3

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–∫–æ–Ω
        if 'correlation_law_deviation' in r:
            corr_score = 1.0 - min(1.0, r['correlation_law_deviation'] / 2.0)
            scores.append(corr_score * 2)  # –í–µ—Å 2

        # –§–ª—É–∫—Ç—É–∞—Ü–∏–∏
        fluct_score = 1.0 - min(1.0, abs(r['connectivity_fluctuation'] - 0.01) / 0.01)
        scores.append(fluct_score * 2)  # –í–µ—Å 2

        return min(10, sum(scores))

    def analyze(self, k_eig=100):
        """–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≥—Ä–∞—Ñ–∞"""
        print("–ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó –ì–†–ê–§–û–í–û–ô –ú–û–î–ï–õ–ò –í–°–ï–õ–ï–ù–ù–û–ô")

        # 1. –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∞
        self.create_graph()

        # 2. –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞
        spectral_gap = self.compute_spectral_properties(k_eig)

        # 3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        entropy_node, entropy_total, connectivity = self.compute_information_metrics()

        # 4. –§–∏–∑–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        d_s_rw, clustering, assortativity = self.compute_physical_metrics()

        # 5. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ï–¢–ò
        self.add_universe_parameters()
        action, corr_exp = self.compute_theoretical_metrics()

        # 6. –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_results()
        self._print_theoretical_interpretation()

        return self.results

    def _print_results(self):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")

        results = self.results

        print(f"–û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        print(f"  –£–∑–ª–æ–≤ (N): {results['N_final']:,}")
        print(f"  –°—Ä–µ–¥–Ω—è—è —Å—Ç–µ–ø–µ–Ω—å: {results['avg_degree']:.2f}")
        print(f"  –†–µ–±–µ—Ä: {results['edges_count']:,}")

        print(f"\n–°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ï –°–í–û–ô–°–¢–í–ê:")
        print(f"  Spectral gap (Œª‚ÇÅ): {results['spectral_gap']:.6f}")
        print(f"  –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['spectral_dimension']:.3f}")
        print(f"  RW —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {results['rw_spectral_dimension']:.3f}")

        print(f"\n–ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"  –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —É–∑–µ–ª: {results['entropy_per_node']:.3f} –±–∏—Ç")
        print(f"  –ü–æ–ª–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è: {results['total_entropy']:.3e} –±–∏—Ç")
        print(f"  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–≤—è–∑–Ω–æ—Å—Ç—å: {results['information_connectivity']:.3f}")

        print(f"\n–§–ò–ó–ò–ß–ï–°–ö–ò–ï –ú–ï–¢–†–ò–ö–ò:")
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: {results['clustering_coefficient']:.4f}")
        print(f"  –ê—Å—Å–æ—Ä—Ç–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: {results['assortativity']:.4f}")
        if 'effective_speed' in results:
            print(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {results['effective_speed']:.4f}")

        print(f"\n–≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –ú–û–î–ï–õ–ò:")
        density = results['edges_count'] / (results['N_final'] * (results['N_final'] - 1) // 2)
        print(f"  –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≥—Ä–∞—Ñ–∞: {density:.6f}")
        print(f"  –≠–Ω—Ç—Ä–æ–ø–∏—è –Ω–∞ —Å–≤—è–∑—å: {results['entropy_per_node'] / results['avg_degree']:.4f} –±–∏—Ç/—Å–≤—è–∑—å")

    def _print_theoretical_interpretation(self):
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è"""
        print("–¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø (–ï–¢–ò –§–û–†–ú–ê–õ–ò–ó–ú)")

        r = self.results

        print(f"–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï –¢–ï–û–†–ï–¢–ò–ß–ï–°–ö–ò–ú –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø–ú:")
        print(f"  ‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å–≤—è–∑–Ω–æ—Å—Ç–∏: {r['k_optimality_ratio']:.3f} (—Ü–µ–ª—å: 1.0)")
        print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {r['spectral_dimension']:.2f} (—Ü–µ–ª—å: {r['target_dimension']:.1f})")
        print(f"  ‚Ä¢ –§–ª—É–∫—Ç—É–∞—Ü–∏–∏ —Å–≤—è–∑–Ω–æ—Å—Ç–∏: {r['connectivity_fluctuation']:.4f} (~‚àö‚Ñè)")

        if 'estimated_correlation_exponent' in r:
            print(f"  ‚Ä¢ –ó–∞–∫–æ–Ω –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: C(r) ‚àù 1/r^{r['estimated_correlation_exponent']:.2f} (—Ü–µ–ª—å: 2.00)")

        print(f"\n–ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–û–ï –î–ï–ô–°–¢–í–ò–ï: {r['informational_action']:.6f}")
        if 'action_components' in r:
            comp = r['action_components']
            print(f"  –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
            print(f"  ‚Ä¢ –ñ–µ—Å—Ç–∫–æ—Å—Ç—å: {comp['rigidity']:.6f}")
            print(f"  ‚Ä¢ –ë–∞–ª–∞–Ω—Å: {comp['balance']:.6f}")
            print(f"  ‚Ä¢ –°–≤–æ–±–æ–¥–∞: {comp['freedom']:.6f}")
            print(f"  ‚Ä¢ –≠–Ω—Ç—Ä–æ–ø–∏—è: {comp['entropy']:.6f}")
            print(f"  ‚Ä¢ –ì–æ–ª–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ: {comp['holographic']:.6f}")

        # –û—Ü–µ–Ω–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –æ—Å–º—ã—Å–ª–µ–Ω–Ω–æ—Å—Ç–∏
        print(f"\n–§–ò–ó–ò–ß–ï–°–ö–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò:")
        quality_score = self._compute_model_quality()
        print(f"  –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {quality_score:.1f}/10")

        if quality_score > 7:
            print("  ‚úÖ –ú–æ–¥–µ–ª—å —Ö–æ—Ä–æ—à–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º")
        elif quality_score > 5:
            print("  ‚ö†Ô∏è  –ú–æ–¥–µ–ª—å —Ç—Ä–µ–±—É–µ—Ç —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        else:
            print("  ‚ùå –ú–æ–¥–µ–ª—å –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–º –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –§–∏–∑–∏—á–µ—Å–∫–∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    N = 3000

    analyzer = UniverseGraphAnalyzer(
        N=N, m=2,
        # 46  - 2.150, 56 - 2.0, 38 - 2.349, 37 - 0, 68 - 1.966, 78 - 1.99, 90 - 2.044, 120 - 2.238
        # 150 - 2.46 170 - 2.628, 195 - 2.827, –≤—Å–µ —á—Ç–æ –≤—ã—à–µ –¥–∞–µ—Ç 0. # 190 - 2.795, 210 - 0, 207 - 0


        # 30000 - 200 - 0. 230 - 0 ,  270. 500 - 1.974, 700 - 1.866, 400 - 2.180, 300 - –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 324 -0, 364- 2.294
        graph_type='WS',
        theoretical_N=1e185,
        theoretical_k=425
    )

    results = analyzer.analyze()

    print("–§–ò–ó–ò–ß–ï–°–ö–ê–Ø –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–Ø –î–õ–Ø –í–°–ï–õ–ï–ù–ù–û–ô:")

    if results['spectral_dimension'] > 0:
        print(f"‚Ä¢ –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: {results['spectral_dimension']:.2f}")
    print(f"‚Ä¢ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –µ–º–∫–æ—Å—Ç—å: {results['total_entropy']:.2e} –±–∏—Ç")
    print(f"‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {results.get('effective_speed', 0):.2f}")
    print(f"‚Ä¢ –°—Ç–µ–ø–µ–Ω—å –∫–≤–∞–Ω—Ç–æ–≤–æ–π –∑–∞–ø—É—Ç–∞–Ω–Ω–æ—Å—Ç–∏: {results['clustering_coefficient']:.3f}")
    print(f"‚Ä¢ –û–ø—Ç–∏–º–∞–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {results['k_optimality_ratio']:.3f}")